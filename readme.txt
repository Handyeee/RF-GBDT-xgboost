决策树算法有很多良好的特性，比如可以直观的展示模型，适用于非线性分类等。但是单决策树容易过拟合，于是人们采用剪枝的办法来尽量减少这种情况。
集成模型的思想（boosting，bagging）与决策树结合，使几百颗简单的决策树组合在一起，组合后的分类器很强大，不容易过拟合。常见的两种基本形式有基于boosting思想的GBDT和基于bagging思想的随机森林（RF）。xgboost是eXtreme Gradient Boosting，xgboost最大的特点在于，它能够自动利用CPU的多线程进行并行，同时在算法上加以改进提高了精度。
　　　本实验在高光谱图像数据上比较了三种基于决策树的算法。
　　　随机森林最简单，计算时间快，可调参数少。
　　　GBDT最慢，调节参数多。
　　　Xgb算法相对GBDT有若干倍的加速比（跟设置并行的线程有关），调节参数最多。
　　　初调效果：xgboost>rf>gbdt
   
  数据下载：链接: https://pan.baidu.com/s/1gfJ4OVL 密码: qxk3
